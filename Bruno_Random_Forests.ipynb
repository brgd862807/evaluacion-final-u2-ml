{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brgd862807/evaluacion-final-u2-ml/blob/main/Bruno_Random_Forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I97Spq48RkFM"
      },
      "source": [
        "# En profundidad: Árboles de decisión y bosques aleatorios"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anteriormente hemos analizado en profundidad un clasificador generativo simple (Naive Bayes; ver [En profundidad: Clasificación Naive Bayes](05.05-Naive-Bayes.ipynb)) y un clasificador discriminativo poderoso (máquinas de vectores de soporte; ver [En profundidad: Máquinas de vectores de soporte](05.07-Support-Vector-Machines.ipynb)).\n",
        "Aquí vamos a examinar otro algoritmo poderoso: un algoritmo no paramétrico llamado *random forests* (bosques aleatorios).\n",
        "\n",
        "Los bosques aleatorios son un ejemplo de un método de *conjunto* (*ensemble*), lo que significa que se basa en agregar los resultados de un conjunto de estimadores más simples.\n",
        "El resultado algo sorprendente de estos métodos de conjunto es que la suma puede ser mayor que las partes: es decir, la precisión predictiva de un voto mayoritario entre varios estimadores puede terminar siendo mejor que la de cualquiera de los estimadores individuales que participan en la votación.\n",
        "\n",
        "Veremos ejemplos de esto en las siguientes secciones.\n",
        "\n",
        "Comenzamos con las importaciones estándar:\n"
      ],
      "metadata": {
        "id": "7Rcj1EmVenCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMqFuLe_Sx-A",
        "outputId": "cf26ca63-1fea-4777-9962-4859ccf4f05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ShZyDGorRkFV"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.style.use('seaborn-whitegrid')\n",
        "sns.set_style(\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miQzcnPrRkFX"
      },
      "source": [
        "## Motivando los Bosques Aleatorios: Árboles de Decisión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ2nJvasRkFZ"
      },
      "source": [
        "Los bosques aleatorios son un ejemplo de un ensemble learner construido a partir de árboles de decisión.\n",
        "Por esta razón, comenzaremos discutiendo los propios árboles de decisión.\n",
        "\n",
        "Los árboles de decisión son formas extremadamente intuitivas de clasificar o etiquetar objetos: simplemente se hace una serie de preguntas diseñadas para llegar a la clasificación correcta.\n",
        "Por ejemplo, si quisieras construir un árbol de decisión para clasificar los animales que encuentras durante una caminata, podrías construir uno como el que se muestra en la figura siguiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg7GmEhCRkFa"
      },
      "source": [
        "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/images/05.08-decision-tree.png?raw=1)\n",
        "[figure source in Appendix](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/06.00-Figure-Code.ipynb#Decision-Tree-Example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TreAyhotRkFb"
      },
      "source": [
        "La división binaria hace que esto sea extremadamente eficiente: en un árbol bien construido, cada pregunta reducirá aproximadamente a la mitad el número de opciones, acotando rápidamente las posibilidades incluso entre un gran número de clases.\n",
        "El truco, por supuesto, consiste en decidir qué preguntas hacer en cada paso.\n",
        "En las implementaciones de árboles de decisión en aprendizaje automático, las preguntas generalmente toman la forma de divisiones alineadas con los ejes de los datos: es decir, cada nodo del árbol divide los datos en dos grupos usando un valor de corte dentro de una de las características.\n",
        "\n",
        "Ahora veamos un ejemplo de esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwidtPVWRkFc"
      },
      "source": [
        "### Creando un Árbol de Decisión\n",
        "\n",
        "Considera los siguientes datos bidimensionales, que tienen una de cuatro etiquetas de clase (ver la figura siguiente):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "TJklfdqgRkFc"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4,\n",
        "                  random_state=0, cluster_std=1.0)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTkTGftMRkFe"
      },
      "source": [
        "Un árbol de decisión simple construido sobre estos datos dividirá iterativamente los datos a lo largo de uno u otro eje según algún criterio cuantitativo, y en cada nivel asignará la etiqueta de la nueva región de acuerdo con un voto mayoritario de los puntos que contiene.\n",
        "La figura siguiente presenta una visualización de los primeros cuatro niveles de un clasificador de árbol de decisión para estos datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1etv6fP3RkFf"
      },
      "source": [
        "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/images/05.08-decision-tree-levels.png?raw=1)\n",
        "[figure source in Appendix](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/06.00-Figure-Code.ipynb#Decision-Tree-Levels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K2xryHCRkFf"
      },
      "source": [
        "Observa que después de la primera división, todos los puntos en la rama superior permanecen sin cambios, por lo que no es necesario subdividir más esta rama.\n",
        "Excepto en los nodos que contienen únicamente un color, en cada nivel *toda* región se vuelve a dividir a lo largo de una de las dos características.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alFZfMXGRkFg"
      },
      "source": [
        "Este proceso de ajustar un árbol de decisión a nuestros datos se puede realizar en Scikit-Learn con el estimador ``DecisionTreeClassifier``:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "rK9yz8IBRkFg"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier().fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDEMnvc2RkFh"
      },
      "source": [
        "Vamos a escribir una función auxiliar para ayudarnos a visualizar la salida del clasificador:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "d80bhPNQRkFh"
      },
      "outputs": [],
      "source": [
        "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
        "    ax = ax or plt.gca()\n",
        "\n",
        "    # Plot the training points\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
        "               clim=(y.min(), y.max()), zorder=3)\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # fit the estimator\n",
        "    model.fit(X, y)\n",
        "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
        "                         np.linspace(*ylim, num=200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    # Create a color plot with the results\n",
        "    n_classes = len(np.unique(y))\n",
        "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
        "                           levels=np.arange(n_classes + 1) - 0.5,\n",
        "                           cmap=cmap, zorder=1)\n",
        "\n",
        "    ax.set(xlim=xlim, ylim=ylim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA5mufoKRkFh"
      },
      "source": [
        "Ahora podemos examinar cómo se ve la clasificación del árbol de decisión (ver la figura siguiente):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "eqi8TYtWRkFi"
      },
      "outputs": [],
      "source": [
        "visualize_classifier(DecisionTreeClassifier(), X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "NezmuvBzRkFi"
      },
      "outputs": [],
      "source": [
        "\n",
        "import helpers\n",
        "helpers.plot_tree_interactive(X, y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERw5S6NLRkFj"
      },
      "source": [
        "Observa que a medida que aumenta la profundidad, tendemos a obtener regiones de clasificación con formas muy extrañas; por ejemplo, a una profundidad de cinco, hay una región púrpura alta y delgada entre las regiones amarilla y azul.\n",
        "Está claro que esto se debe menos a la verdadera distribución intrínseca de los datos y más a las propiedades particulares del muestreo o al ruido presente en los datos.\n",
        "Es decir, este árbol de decisión, incluso con solo cinco niveles de profundidad, está claramente sobreajustando nuestros datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX4QX9YGRkFk"
      },
      "source": [
        "### Árboles de Decisión y Sobreajuste\n",
        "\n",
        "Este sobreajuste resulta ser una propiedad general de los árboles de decisión: es muy fácil profundizar demasiado en el árbol y, por lo tanto, ajustar detalles de los datos particulares en lugar de las propiedades generales de las distribuciones de las que provienen.\n",
        "Otra forma de observar este sobreajuste es mirar modelos entrenados con diferentes subconjuntos de los datos; por ejemplo, en esta figura entrenamos dos árboles diferentes, cada uno con la mitad de los datos originales.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAOWhfYlRkFk"
      },
      "source": [
        "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/images/05.08-decision-tree-overfitting.png?raw=1)\n",
        "[figure source in Appendix](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/06.00-Figure-Code.ipynb#Decision-Tree-Overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGwDK8OzRkFk"
      },
      "source": [
        "Está claro que en algunos lugares los dos árboles producen resultados consistentes (por ejemplo, en las cuatro esquinas), mientras que en otros lugares los dos árboles ofrecen clasificaciones muy diferentes (por ejemplo, en las regiones entre dos grupos de puntos).\n",
        "La observación clave es que las inconsistencias tienden a ocurrir donde la clasificación es menos segura, y por lo tanto, al usar información de *ambos* árboles, ¡podríamos obtener un mejor resultado!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUNxxuWsRkFl"
      },
      "source": [
        "Si estás ejecutando este cuaderno en vivo, la siguiente función te permitirá mostrar de manera interactiva los ajustes de los árboles entrenados con un subconjunto aleatorio de los datos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "BLUvO9BNRkFl"
      },
      "outputs": [],
      "source": [
        "# helpers_05_08 is found in the online appendix\n",
        "import helpers\n",
        "helpers.randomized_tree_interactive(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRrPkpzCRkFm"
      },
      "source": [
        "Así como usar información de dos árboles mejora nuestros resultados, podríamos esperar que usar información de muchos árboles los mejore aún más.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7WAex_7RkFm"
      },
      "source": [
        "## Conjuntos de Estimadores: Bosques Aleatorios\n",
        "\n",
        "Esta idea—que múltiples estimadores con sobreajuste pueden combinarse para reducir el efecto de este sobreajuste—es la base de un método de conjunto llamado *bagging*.\n",
        "Bagging utiliza un conjunto (quizás una “bolsa variada”) de estimadores paralelos, cada uno de los cuales sobreajusta los datos, y promedia los resultados para obtener una mejor clasificación.\n",
        "Un conjunto de árboles de decisión aleatorizados se conoce como *bosque aleatorio* (*random forest*).\n",
        "\n",
        "Este tipo de clasificación por bagging se puede realizar manualmente usando el meta-estimador `BaggingClassifier` de Scikit-Learn, como se muestra aquí (ver la figura siguiente):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "UQduIjvqRkFn"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n",
        "                        random_state=1)\n",
        "\n",
        "bag.fit(X, y)\n",
        "visualize_classifier(bag, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo, hemos aleatorizado los datos ajustando cada estimador con un subconjunto aleatorio del 80 % de los puntos de entrenamiento.\n",
        "En la práctica, los árboles de decisión se aleatorizan de manera más efectiva introduciendo cierta aleatoriedad en cómo se eligen las divisiones: de esta manera, todos los datos contribuyen al ajuste cada vez, pero los resultados del ajuste mantienen la aleatoriedad deseada.\n",
        "Por ejemplo, al determinar en qué característica dividir, el árbol aleatorizado podría seleccionar entre varias de las mejores características.\n",
        "Puedes leer más detalles técnicos sobre estas estrategias de aleatorización en la [documentación de Scikit-Learn](http://scikit-learn.org/stable/modules/ensemble.html#forest) y en las referencias allí incluidas.\n",
        "\n",
        "En Scikit-Learn, un conjunto optimizado de árboles de decisión aleatorizados se implementa en el estimador `RandomForestClassifier`, que se encarga automáticamente de toda la aleatorización.\n",
        "Todo lo que necesitas hacer es seleccionar un número de estimadores, y rápidamente—en paralelo, si se desea—ajustará el conjunto de árboles (ver la figura siguiente):\n"
      ],
      "metadata": {
        "id": "P3rYXKSpgW0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "gV5odZW6RkFo"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "visualize_classifier(model, X, y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GTLvrliRkFp"
      },
      "source": [
        "Vemos que al promediar sobre 100 modelos perturbados aleatoriamente, obtenemos un modelo global que se aproxima mucho más a nuestra intuición sobre cómo debería dividirse el espacio de parámetros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ak9X9K3RkFp"
      },
      "source": [
        "## Regresión con Bosques Aleatorio\n",
        "\n",
        "En la sección anterior consideramos los bosques aleatorios en el contexto de la clasificación.\n",
        "Los bosques aleatorios también pueden aplicarse en el caso de regresión (es decir, con variables continuas en lugar de categóricas). El estimador a utilizar para esto es `RandomForestRegressor`, y la sintaxis es muy similar a la que vimos anteriormente.\n",
        "\n",
        "Considera los siguientes datos, obtenidos de la combinación de una oscilación rápida y otra lenta (ver la figura siguiente):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "S2f7nicgRkFq"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(42)\n",
        "x = 10 * rng.rand(200)\n",
        "\n",
        "def model(x, sigma=0.3):\n",
        "    fast_oscillation = np.sin(5 * x)\n",
        "    slow_oscillation = np.sin(0.5 * x)\n",
        "    noise = sigma * rng.randn(len(x))\n",
        "\n",
        "    return slow_oscillation + fast_oscillation + noise\n",
        "\n",
        "y = model(x)\n",
        "plt.errorbar(x, y, 0.3, fmt='o');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C32Lg_otRkFq"
      },
      "source": [
        "Usando el regresor de bosques aleatorios, podemos encontrar la curva de mejor ajuste de la siguiente manera (ver la figura siguiente):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "7Gll34OoRkFr"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest = RandomForestRegressor(200)\n",
        "forest.fit(x[:, None], y)\n",
        "\n",
        "xfit = np.linspace(0, 10, 1000)\n",
        "yfit = forest.predict(xfit[:, None])\n",
        "ytrue = model(xfit, sigma=0)\n",
        "\n",
        "plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\n",
        "plt.plot(xfit, yfit, '-r');\n",
        "plt.plot(xfit, ytrue, '-k', alpha=0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rvzR49BRkFr"
      },
      "source": [
        "Aquí, el modelo verdadero se muestra con la curva gris suave, mientras que el modelo de bosque aleatorio se muestra con la curva roja irregular.\n",
        "El modelo no paramétrico de bosque aleatorio es lo suficientemente flexible como para ajustarse a los datos multiperiodo, ¡sin que necesitemos especificar un modelo multiperiodo!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG9-TMtrRkFs"
      },
      "source": [
        "## Ejemplo: Bosque Aleatorio para Clasificar Dígitos\n",
        "\n",
        "En el Capítulo 38 trabajamos un ejemplo usando el conjunto de datos *digits* incluido en Scikit-Learn.\n",
        "Vamos a usarlo de nuevo aquí para ver cómo se puede aplicar el clasificador de bosque aleatorio en este contexto:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "tEHurSdoRkFs"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5gP1GFPRkFz"
      },
      "source": [
        "Para recordarnos qué estamos observando, vamos a visualizar los primeros puntos de datos (ver la figura siguiente):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "pk5THAuaRkFz"
      },
      "outputs": [],
      "source": [
        "# set up the figure\n",
        "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# plot the digits: each image is 8x8 pixels\n",
        "for i in range(64):\n",
        "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
        "\n",
        "    # label the image with the target value\n",
        "    ax.text(0, 7, str(digits.target[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p31LHuRERkF0"
      },
      "source": [
        "Podemos clasificar los dígitos usando un bosque aleatorio de la siguiente manera:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "0StsoOtpRkF0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
        "                                                random_state=0)\n",
        "model = RandomForestClassifier(n_estimators=1000)\n",
        "model.fit(Xtrain, ytrain)\n",
        "ypred = model.predict(Xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbmQ4k5nRkF0"
      },
      "source": [
        "Veamos el informe de clasificación de este clasificador:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "nQV3ghCZRkF1"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(ypred, ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja9gT037RkF1"
      },
      "source": [
        "Y, para mayor claridad, grafiquemos la matriz de confusión (ver la figura siguiente):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "hXw9RgUnRkF1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "mat = confusion_matrix(ytest, ypred)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
        "            cbar=False, cmap='Blues')\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6XtnxJ0RkF2"
      },
      "source": [
        "Encontramos que un bosque aleatorio simple, sin ajuste, resulta en una clasificación bastante precisa del conjunto de datos de dígitos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IziSif98RkF2"
      },
      "source": [
        "## Resumen\n",
        "\n",
        "Este capítulo proporcionó una breve introducción al concepto de estimadores de conjunto, y en particular al bosque aleatorio, un conjunto de árboles de decisión aleatorizados.\n",
        "Los bosques aleatorios son un método poderoso con varias ventajas:\n",
        "\n",
        "* Tanto el entrenamiento como la predicción son muy rápidos, debido a la simplicidad de los árboles de decisión subyacentes. Además, ambas tareas se pueden paralelizar de manera sencilla, ya que los árboles individuales son entidades completamente independientes.\n",
        "* Los múltiples árboles permiten una clasificación probabilística: un voto mayoritario entre los estimadores proporciona una estimación de la probabilidad (accesible en Scikit-Learn con el método `predict_proba`).\n",
        "* El modelo no paramétrico es extremadamente flexible y, por lo tanto, puede desempeñarse bien en tareas que otros estimadores tienden a subajustar.\n",
        "\n",
        "Una desventaja principal de los bosques aleatorios es que los resultados no son fácilmente interpretables: es decir, si deseas sacar conclusiones sobre el *significado* del modelo de clasificación, los bosques aleatorios pueden no ser la mejor opción.\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}